---
title: "Methods for Recommending and Analyzing Movies"
author: "Thomas Theisen"
date: "December 18, 2017"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##Introduction

Recommender systems have proliferated in recent years with respect to the exponential amount of data generated by users like you and I. Companies like Netflix and Facebook use recommendation systems to predict the likelihood a user will watch a movie or click on an ad. Under the umbrella of recommendation systems sit two primary methods of prediction: Collaborative Filtering and Content-Based Filtering. In the following report, movie rating data, provided by Grouplens.org, will be used to explore the methods mentioned previously. In addition, movie information from IMDB.com will be utilized to determine if the words contained in a movie's plot can classify the genre in which the movie falls. 

##Data Description

```{r load data, echo = FALSE, warning = FALSE, message = FALSE}
setwd("~/Project Portfolio/R/Movie Analysis")

links <- read.csv('links.csv', header = TRUE)
movies <- read.csv('movies.csv', header = TRUE)
ratings <- read.csv('ratings.csv', header = TRUE)
info <- read.csv('all_movie_information.csv', header = TRUE)

library(omdbapi)
library(tidyverse)
library(dplyr)
library(doParallel)
library(ggplot2)
library(reshape2)
library(stringr)
library(recommenderlab)
library(tm)
library(e1071)
library(RTextTools)
library(tidytext)
library(plotly)
library(clustrd)
library(proxy)
library(lsa)
library(class)
library(AUC)
```

```{r, echo = FALSE, message = FALSE, warning = FALSE}
movie_genres_list = c('Action','Adventure','Animation','Children','Comedy','Crime','Documentary','Drama',
                      'Fantasy','Film-Noir','Horror','Musical','Mystery','Romance','Sci-Fi','Thriller','War','Western','(no genres listed)')

movies <- movies %>% mutate(Action = 0, Adventure = 0, Animation = 0, Children = 0, Comedy = 0, Crime = 0,
                            Documentary = 0, Drama = 0, Fantasy = 0, Film_Noir = 0, Horror = 0, Musical = 0,
                            Mystery = 0, Romance = 0, Sci_Fi = 0, Thriller = 0, War = 0, Western = 0, None = 0)

for(row in 1:nrow(movies)){
  
  genre_string = as.character(movies$genres[row])
  
  for(g in 1:length(movie_genres_list)){
    
    check_level = movie_genres_list[g]
    in_string = grep(check_level,genre_string)
    
    if(length(in_string) > 0){
      movies[row,g+3] = 1
    }
  } 
}
```

There were three primary datasets used in this analysis. The "ratings" dataset contains 100,000 observations, where each row is comprised of a user id, movie id, and the rating given by the user for the movie. All 671 unique users had rated at least 20 movies. A sample of the ratings dataset is shown below:

```{r, echo = FALSE, warning=FALSE, message=FALSE}
as.matrix(head(ratings[,1:3],1))
```

The "movies" dataset contains the information on 9,125 movies. This information includes a movie id, movie title, movie genre, and the movie release date.

```{r, echo = FALSE, warning=FALSE, message=FALSE}
as.matrix(head(movies[,c(1,2,3)],1))
```

The "links" dataset was used to extract additional information from the IMDB API. Each movie had a corresponding imdb id, which allowed for the opportunity to gather additional information such as the movie's rating, runtime, director, writer, actors, plot, and imdbRating. This information was stored in a new data frame called "info". 

```{r, echo = FALSE, warning = FALSE, message = FALSE}
collect_movie_info = function(row_low,row_high){
  library(doParallel)
  library(dplyr)
  cluster = makeCluster(3)
  registerDoParallel(cluster)
  links <- read.csv('links.csv', header = TRUE)
  movie_info = foreach(i = row_low:row_high, .combine = rbind, .packages = c('omdbapi','dplyr','stringr')) %dopar%{
    movie_data = { tryCatch(
        {
        check_nchar = nchar(links$imdbId[i])
        print(check_nchar)
        Id = links$movieId[i]
        if(check_nchar == 7){
          id_string = paste0("tt",links$imdbId[i])
          temp = find_by_id(id_string)
          info = data.frame(cbind(Id,temp))[1,]
        }else{
          replace = 7 - check_nchar 
          needed_chars = str_pad(0,replace,pad = "0")
          first_bit = paste0("tt",needed_chars)
          id_string = paste0(first_bit,links$imdbId[i])
          temp = find_by_id(id_string)
          info = data.frame(cbind(Id,temp))[1,]
        }
        info_select = info %>% select(Id,Title,Year,Rated,Runtime,Director,Writer,Actors,Plot,imdbRating,imdbVotes)
        return(info_select)
        
      },error = function(e){ 
        print("error") 
      }
      )}
    }
  stopCluster(cluster)
  return(movie_info)
}
#collect_movie_info(1:nrow(links))
```

```{r, echo = FALSE, warning=FALSE, message=FALSE}
user_stats = ratings %>% group_by(userId) %>%
  summarise(Num_Ratings = n(), Min_Rating = min(rating), Max_Rating = max(rating),
            Mean_Rating = mean(rating), SD_rating = sd(rating)) %>%
  arrange(desc(Mean_Rating))

joint = ratings %>% inner_join(movies, by = 'movieId')
user_movie_stats = joint %>% group_by(userId) %>%
  summarise(Action.Mean = mean(rating[Action == 1]), Action.Obs = sum(Action == 1),
            Adventure.Mean = mean(rating[Adventure == 1]), Adventure.Obs = sum(Adventure == 1),
            Animation.Mean = mean(rating[Animation == 1]), Animation.Obs = sum(Animation == 1),
            Children.Mean = mean(rating[Children == 1]), Children.Obs = sum(Children == 1),
            Comedy.Mean = mean(rating[Comedy == 1]), Comedy.Obs = sum(Comedy == 1),
            Crime.Mean = mean(rating[Crime == 1]), Crime.Obs = sum(Crime == 1),
            Documentary.Mean = mean(rating[Documentary == 1]), Documentary.Obs = sum(Documentary == 1),
            Drama.Mean = mean(rating[Drama == 1]), Drama.Obs = sum(Drama == 1),
            Fantasy.Mean = mean(rating[Fantasy == 1]), Fantasy.Obs = sum(Fantasy == 1),
            Film_Noir.Mean = mean(rating[Film_Noir == 1]), Film_Noir.Obs = sum(Film_Noir == 1),
            Horror.Mean = mean(rating[Horror == 1]), Horror.Obs = sum(Horror == 1),
            Musical.Mean = mean(rating[Musical == 1]), Musical.Obs = sum(Musical == 1),
            Mystery.Mean = mean(rating[Mystery == 1]), Mystery.Obs = sum(Mystery == 1),
            Romance.Mean = mean(rating[Romance == 1]), Romance.Obs = sum(Romance == 1),
            Sci_Fi.Mean = mean(rating[Sci_Fi == 1]), Sci_Fi.Obs = sum(Sci_Fi == 1),
            Thriller.Mean = mean(rating[Thriller == 1]), Thriller.Obs = sum(Thriller == 1),
            War.Mean = mean(rating[War == 1]), War.Obs = sum(War == 1),
            Western.Mean = mean(rating[Western == 1]), Western.Obs = sum(Western == 1))
genre_stats = data.frame(apply(user_movie_stats[,-1],2,mean,na.rm=TRUE))
colnames(genre_stats) = 'Values'
mean_index = grep("Mean",rownames(genre_stats))
obs_index = grep("Obs",rownames(genre_stats))            
genre_stats_df = data.frame(genre_stats$Values[mean_index],genre_stats$Values[obs_index])
colnames(genre_stats_df) = c('Means','Observations')
rownames(genre_stats_df) = movie_genres_list[-19]
sum_movie_genres = apply(movies[,4:21],2,sum)
```

The chart below shows that movies categorized as 'Dramas' are on average the most watched genre across all users. The size of each data point describes the number of total movies with that genre specification. Therefore, it makes sense that the most available genre of movies is also the most watched on average. Although documentary, war, and film-noir movies are watched less than 10 times by users on average, they maintain the highest average rating. Horror movies are by far the lowest rated genre. The primary difficulty faced while using this data was how a movies genre was specified. Very few movies are described using a single genre like "Action", but instead, a combination of genres such as "Action, Romance, War". In total, there are 902 unique genre combinations, with the large majority of combinations only occuring once. Therefore, the average ratings on the chart below are not strictly a single genre such as "Action", but the average rating of all movies that contained "Action" somewhere in the genre list. That is, each genres final average rating is influenced by external genres. 

```{r, echo = FALSE, fig.width=10, fig.height=3.5, warning=FALSE, message=FALSE}
ggplot(data = genre_stats_df, aes(x = Observations, y = Means)) +
  geom_point(size = sum_movie_genres/500, alpha = .5, colour = 'green') + geom_text(aes(label = rownames(genre_stats_df)),hjust=.5, vjust=1.0) +
  xlab('Average Number of Observations by Users') + ylab('Average Rating') + theme_bw()
```

Below is a histogram describing the movie ratings given in the ratings dataset. It seems people are more likely to rate a movie on a whole number basis. The most observed ratings are 4.0, 3.0, and 5.0.

```{r, echo = FALSE, fig.width=10, fig.height=2, warning=FALSE, message=FALSE}
d_hist = ratings %>% group_by(rating) %>%summarise(Counts = n())
ggplot(data = d_hist, aes(x = rating, y = Counts)) + geom_bar(stat = 'identity') + xlab('Rating') +ggtitle('Ratings Histogram') + theme_minimal()
```

##Explicit Recommedation Systems
###User Rating Bias

Before beginning the recommendation process, a few pre-processing measures can significantly improve movie recommendations.
Significant improvements in predictions can be made by normalizing user ratings to remove user bias. For example, person A might be a real movie critic who considers a rating of 3 to be quite good, reserving ratings above 3 for outstanding movies. Person B may be a casual observer who might give a rating of 4 if the movie is really bad, otherwise almost always a 5. This normalization process removes an individual users personal scale for what he or she considers good versus bad in terms of rating. The two popular methods of normalization are 'centering' where the average rating for all movies given by user u is subtracted from each individual rating and Z-score normalization. When computing recommendations in this analysis, the Z-score normalization method was used to account for variance in a users ratings. Below is a histogram describing the left skewed distribution of the normalized user ratings.

```{r, echo = FALSE, warning = FALSE, message = FALSE, fig.height=3}
r = ratings %>% group_by(movieId) %>% summarise(Occurs = n()) %>% filter(Occurs > 25)
l = r$movieId
ll = which(ratings$movieId %in% l)
ratings = ratings[ll,]
rating_matrix = dcast(ratings, userId ~ movieId, value.var = 'rating', na.rm = FALSE);rating_matrix = as.matrix(rating_matrix[,-1])
rating_matrix = as(rating_matrix, "realRatingMatrix")
hist(getRatings(normalize(rating_matrix)), breaks = 100, freq = TRUE, col = 'blue', xlab = "Normalized Rating", main = 'Normalized Ratings Histogram')
```

###Item Based versus User Based Collaborative Filtering

Item based collaborative filtering is an algorithm-based method of producing recommendations calculated by assessing the relationship between items (movies) given in the ratings dataset. The user based collaborative filtering method is identical in computation except that recommendations are generated based on the relationship between users. By transforming the ratings dataset from a long format to a wide format, a matrix is created where each user is a row, and the columns represent every possible movie the user could have rated. The ratings matrix itself was quite sparse because users on average watched only 149 movies of the possible 9,125. Due to memory issues within R, I had to remove any movies that had been watched less than 25 total times.

The three most popular similiarity computation methods are cosine similiarity, pearson correlation-based similiarity, and Jaccard similiarity. Cosine similiarity is calculated by taking two vectors and calculating the cosine of the angle between them. The correlation method is much the same, expect the correlation between the two vectors is computed. Jaccard similiarity is calculated as the cardinality of the intersection of two vectors divided by the union of those two vectors. Larger final values for any of these computations are indicitive of a higher similiarity between vectors.

In item based collaborative filtering, these two vectors are comprised of users who have watched both movies x and y. User based collaborative filtering uses the opposite approach, where the two vectors are based on each movie both user a and b have watched. Ultimately, the similarity between each pair of movies (pair of users) in the ratings dataset will be computed in order to begin recommending movies the users haven't rated before. 

In both item and user based collaborative filtering the parameter k or n are used to dictate the number of most similar movies or users used in the recommendation computation, respectively. These parameters are tuned to optimize recommendations based upon the dataset. In order to demonstrate the process of collaborative filtering, the R package "recommenderlab" will be used. 

Below are the top 3 recommended movies for user 2. The results of item and user based collaborative filtering are given in order below.

```{r, echo = FALSE, message = FALSE, warning = FALSE}
rating_matrix_normalized = normalize(rating_matrix, method = 'Z-score')
item_based_model = Recommender(rating_matrix_normalized, method = "IBCF", param = list(method = "Cosine", k = 5))
recommendation_item = predict(item_based_model, rating_matrix[2], n = 3)
recom_list_item = as(recommendation_item, "list")
r_i = which(movies$movieId %in% recom_list_item[[1]])
movies[r_i,1:3]
```

```{r, echo = FALSE, message = FALSE, warning = FALSE}
user_based_model = Recommender(rating_matrix_normalized, method = "UBCF", param = list(method = "Cosine", nn = 30))
recommendation_user = predict(user_based_model, rating_matrix[2], n = 3)
recom_list_user = as(recommendation_user, "list")
r_i = which(movies$movieId %in% recom_list_user[[1]])
movies[r_i,1:3]
```

###Accessing Error Between Collaborative Filtering Methods

The three most popular metrics of assessing recommendation system error are root mean squared error, mean square error, and mean absolute error. Using cross validation as a method of computing these metrics, the error statistics for user and item based collaborative filtering are given below including the similiarity method used to compute them. Overall, it appears the cosine similiarity, user based methodology is the most appropriate method for recommending movies for users in this dataset.

```{r, echo = FALSE, message = FALSE, warning = FALSE}
es <- evaluationScheme(data = rating_matrix, method = "cross-validation", train = 0.8, given = -1, goodRating = 3)
rec.ubcf.cos = Recommender(getData(es, "train"), method  = "UBCF", param = list(method = "Cosine", nn = 3))
rec.ibcf.cos = Recommender(getData(es, "train"), method = "IBCF", param = list(method = "Cosine", k = 30))
rec.ubcf.cor = Recommender(getData(es, "train"), method  = "UBCF", param = list(method = "Pearson", nn = 3))
rec.ibcf.cor = Recommender(getData(es, "train"), method = "IBCF", param = list(method = "Pearson", k = 30))
rec.ubcf.j = Recommender(getData(es, "train"), method  = "UBCF", param = list(method = "Jaccard", nn = 3))
rec.ibcf.j = Recommender(getData(es, "train"), method = "IBCF", param = list(method = "Jaccard", k = 30))
p.ubcf.cos <- predict(rec.ubcf.cos, getData(es, "known"), type="ratings")
p.ibcf.cos <- predict(rec.ibcf.cos, getData(es, "known"), type="ratings")
p.ubcf.cor <- predict(rec.ubcf.cor, getData(es, "known"), type="ratings")
p.ibcf.cor <- predict(rec.ibcf.cor, getData(es, "known"), type="ratings")
p.ubcf.j <- predict(rec.ubcf.j, getData(es, "known"), type="ratings")
p.ibcf.j <- predict(rec.ibcf.j, getData(es, "known"), type="ratings")
error.ubcf.cos = calcPredictionAccuracy(p.ubcf.cos, getData(es, "unknown"))
error.ibcf.cos = calcPredictionAccuracy(p.ibcf.cos, getData(es, "unknown"))
error.ubcf.cor = calcPredictionAccuracy(p.ubcf.cor, getData(es, "unknown"))
error.ibcf.cor = calcPredictionAccuracy(p.ibcf.cor, getData(es, "unknown"))
error.ubcf.j = calcPredictionAccuracy(p.ubcf.j, getData(es, "unknown"))
error.ibcf.j = calcPredictionAccuracy(p.ibcf.j, getData(es, "unknown"))
error_joint = rbind(error.ubcf.cos,error.ibcf.cos,error.ubcf.cor,error.ibcf.cor,error.ubcf.j,error.ibcf.j);rownames(error_joint) = c("UBCF-COS","IBCF-COS","UBCF-COR","IBCF-COR","UBCF-Jaccard","IBCF-Jaccard")
```

```{r, echo = FALSE, fig.width=10, fig.height=3.0}
melted_errors = data.frame(melt(error_joint))
ggplot(data = melted_errors, aes(x = Var1, y = value)) + 
  geom_bar(stat = 'identity', aes(fill = Var2), position = 'dodge') +
  xlab('Recommendation Method') + ylab('Error') + scale_fill_discrete(name = "Error Metric") + ggtitle('Error Metrics by Method')
```

###Problems with Explicit Recommendation Systems and a Case for Content-Based Recommendation Models

Content-based systems use metadata or the properties of items to generate similarities. For movies these properties include details like release date, directors, writers, and actors. The dataset "info" contains this information, but the time needed to clean and transform this data into usable form is substantial giving the varying number of directors, writers, and actors in each movie. Thus, I am leaving this up to future work and research.

Content based recommendation models are generally considered superior to collaborative filtering methods for the following reasons: The results tend be highly relevant and transparent due to the similiarities between movie properties and not explicit movie ratings, and new movies can be recommended immediately because the properties of the new movie are known. Collaborative filtering recommendations give no direct indication to why a given recommendation may be relavent to a given user, as witnessed in the recommendations above. User 2 was given multiple recommendations that were action heavy, when in fact he or she rates western, war, crime, and animation movies more highly.

The similarity properties that drive true and understandable recommendations are those where plot descriptions are similiar, actors or directors overlap, or similiar genres are reported. As many researchers say, collaborative filtering gives 'dumb' recommendations. Content based modeling gives Netflix the opportunity to list relavent '1980's mystery romances' that are similiar to other '1980's mystery romances' a user has already watched. Out-of-the-box recommendations coming from companies like Netflix utilize an off-by-one approach where an individuals love for '1980's mystery romances' might suggest to Netflix this individual will enjoy '1980's mystery thrillers' as well. 

##Classifying Movie Genres using Movie Plot Descriptions

To begin the process of movie genre classification, pre-modeling steps such as string cleaning and data structure alterations were necessary. The first step in the process was converting all the plot descriptions to lower case, removing all punctuation, and any 1 or 2 letter words. Next, "stopwords" were removed. Stopwords are high frequency words that create minimal additional meaning to most sentences. Next, each individual word in each plot description was checked to see if it could be reduced down to a root word through a process known as stemming. For example, "accounts" would become "account". Next, the data frame structure was coerced into a structure known as a document term matrix. This structure is formatted to allocate a single row to each movie, where 1's or 0's are denoted on the columns as a method to indicate the presence of a word in the movie's plot description. To further reduce the number of word features, any word that didn't occur more than 25 times throughout all movie plots was removed. Future research into the appropriate word frequency cutoff would be interesting. After this whole process, 1096 words remained. A glimpse of the document term matrix is given below.

```{r,, echo = FALSE, warning = FALSE, message = FALSE}
error_index = which(info$Title == 'error')
info = info[-error_index,]

#Changes to string structure
info$Plot = tolower(info$Plot)
info$Plot = str_replace_all(info$Plot, "[^[:alnum:]]", " ")
info$Plot = gsub('[[:digit:]]+', '', info$Plot)
info$Plot <- gsub(" *\\b[[:alpha:]]{1,2}\\b *", " ", info$Plot)
stopWords <- stopwords("en")
info$Plot = removeWords(info$Plot,stopWords)
movies_remaining = as.numeric(as.character(info$Id))

#Make corpus from movie plots and apply stemming
corpus = Corpus(VectorSource(info$Plot))
corpus_stem= tm_map(corpus, stemDocument, "english")
dtm = DocumentTermMatrix(corpus_stem)

#Remove words from dtm that occur fewer than n times
freqLevel = findFreqTerms(dtm,25)
dtm_controlled = DocumentTermMatrix(corpus, control = list(dictionary = freqLevel))
as.matrix(dtm_controlled[114,1:10])
```

Given the finalized data structure, factor analysis (using a 'varimax' rotation) was used as a dimensionality reduction method to create factor loadings for each word. The cumulative variance explained by the first three loadings was only 0.006, which is evident in the plot below. Future research regarding methods of dimensionality reduction that could bolster the amount of cumulative variance explained could lead to significantly better genre predictions. Next, the factor loadings were supplied to the kmeans algorithm to create latent variables or "superwords". A scree plot indicates roughly 10 clusters are appropriate. That is, each of the 10 superwords is comprised of a collection of individuals words that were closest to one another in the feature space. It's interesting to see the word pairs high school, teacher student, senior popular, and san francisco were correctly grouped together, in reference to our own knowledge of pairs of words that are most closely associated in everyday life. Otherwise, every word had factor loadings, in two dimensional space, between -0.125 and 0.125 on the x-axis and -0.125 and 0.10 on the y-axis. 

```{r, echo = FALSE, warning = FALSE, message = FALSE, fig.width=10}
set.seed(12345)
DF <- data.frame(as.matrix(dtm_controlled), stringsAsFactors=FALSE)
DF$movieId = movies_remaining
long = melt(DF, id.vars = c("movieId"))
long_form = long %>% filter(value > 0) %>% arrange(movieId)
DF_Cast = acast(long_form, movieId ~ variable)
DF_Cast = ifelse(is.na(DF_Cast), 0, 1)
fit = factanal(DF_Cast, 3, rotation = 'varimax')
loadings = fit$loadings[,1:3]

#accumulator for cost results
cost_df <- data.frame()

#run kmeans for all clusters up to 100
for(i in 1:100){
  #Run kmeans for each level of i, allowing up to 100 iterations for convergence
  kmeans<- kmeans(x=loadings, centers=i, iter.max=100)
  
  #Combine cluster number and cost together, write to df
  cost_df<- rbind(cost_df, cbind(i, kmeans$tot.withinss))

}
names(cost_df) <- c("cluster", "cost")

k_clusters = 10
K_results_color = kmeans(loadings,k_clusters, iter.max = 25)
df_loadings = data.frame(cbind(loadings,K_results_color$cluster),rownames(loadings))
g = ggplot(df_loadings, aes(x = Factor1, y = Factor2)) + geom_point() +
  geom_text(aes(label = rownames(df_loadings), color = factor(V4))) 
g + theme(legend.position="none") 
```

To create the final data structure, a new data frame was created to denote which superwords occured in each movie plots description. 

```{r, echo = FALSE, warning = FALSE, message = FALSE}
df_loadings$term = rownames(df_loadings)
colnames(long_form) = c('movieId','term','value')
together = long_form %>% left_join(df_loadings, by = 'term')
together_one_hot = as.data.frame(acast(together, movieId ~ V4))
together_one_hot$movieId = as.numeric(rownames(together_one_hot))
bound = together_one_hot %>% left_join(movies, by = 'movieId')
sup_names = rep(paste0('Sw',1:k_clusters))
colnames(bound) = c(sup_names,
                    'movieId','title','genres','Action','Adventure','Animation','Children','Comedy',
                    'Crime','Documentary','Drama','Fantasy','Film_Noir','Horror','Musical','Mystery',
                    'Romance','Sci_Fi','Thriller','War','Western','None')
bound = bound[,-ncol(bound)]
```

```{r, echo = FALSE, warning = FALSE, message = FALSE}
set.seed(123)
smp_size = floor(0.75 * nrow(bound))
train_index = sample(seq_len(nrow(bound)), size = smp_size)
train = bound[train_index,]
test = bound[-train_index,]
bound[,1:10] = ifelse(bound[,1:10] > 0, 1, 0)

sw_pos = grep("Sw",names(bound))
genre_first = grep('Action', names(bound))
genre_last = grep('Western', names(bound))
store_values = data.frame(matrix(ncol = 5, nrow = 1800))
colnames(store_values) = c('Threshold','Genre','Acc','Sens','Spec')
count = 1
for(genre_i in genre_first:genre_last){
  subset = train[,c(genre_i,sw_pos)]
  model = naiveBayes(subset[,1] ~ ., data = subset, laplace = 1)
  predictions = as.data.frame(predict(model, test[,1:10], type = 'raw'))
  colnames(predictions) = c('No','Yes')
    for(threshold in 1:100){
      predictions$Choice = ifelse(predictions$Yes >= (threshold/100),1,0)
      cm = table(test[,genre_i],predictions$Choice)
      if(dim(cm)[2] == 2){
        store_values$Threshold[count] = threshold
        store_values$Genre[count] = colnames(train[genre_i])
        store_values$Acc[count] = (cm[1,1]+cm[2,2])/(cm[1,1]+cm[1,2]+cm[2,1]+cm[2,2])
        store_values$Sens[count] = (cm[2,2]/(cm[2,2] + cm[2,1]))
        store_values$Spec[count] = (cm[1,1]/(cm[1,1] + cm[1,2]))
      }else{
        store_values$Threshold[count] = threshold
        store_values$Genre[count] = colnames(train[genre_i])
        store_values$Acc[count] = 0
        store_values$Sens[count] = 0
        store_values$Spec[count] = 0
      }
      count = count + 1
    }
}
store_values = store_values %>% mutate(OneMinusSpec = 1 - Spec, Maximum = Sens + Spec + 1)
high_accuracy_index = store_values %>% group_by(Genre) %>% slice(which.max(Maximum))
```

The naive bayes algorithm was used as the method of classification. This classification techinque is based on Baye's Theorem with an assumption of independence among predictors. That is, each superword feature is regarded as unrelated to the presence of any other superword. Laplace estimation is utilized as a smoothing techinque to solve the problem of any zero frequency features between training and test sets. Individual naive bayes models are created for each genre. That is, the results from 18 separate naive bayes models are used to predict the final genre classification vector for each movie. Model probability cutoffs were decided upon for each model by choosing the cutoff value that returned the highest testing accuracy. 

Final model classification accuracy was slightly underwhelming. The final model resolved to an over classification (rate) error metric of 1.96, and an under classification metric of 1.76. That means the model was predicting roughly the correct number of listed genres for each movie, but often the wrong genres. Only 20.7% of movies were predicted perfectly. In total, the model predicted at least 1 genre correctly 47.9% of the time. These percentages fluctuate when a seed isn't set before using k-means. 

```{r, echo = FALSE, warning = FALSE, message = FALSE}
sw_pos = grep("Sw",names(bound))
genre_first = grep('Action', names(bound))
genre_last = grep('Western', names(bound))
cutoffs = high_accuracy_index$Threshold
predictions_df = data.frame(matrix(ncol = 18, nrow = nrow(test)))
j = 1
for(genre_i in genre_first:genre_last){
  subset = train[,c(genre_i,sw_pos)]
  model = naiveBayes(subset[,1] ~ ., data = subset, laplace = 1)
  predictions = as.data.frame(predict(model, test[,1:10], type = 'raw'))
  colnames(predictions) = c('No','Yes')
  predictions$Choice = ifelse(predictions$Yes >= ((cutoffs[j]/100)+.1),1,0)
  predictions_df[,j] = predictions$Choice
  j = j + 1
}
actual_genres = test[,genre_first:genre_last]
actual_genres = ifelse(actual_genres == 1, 10,0)
error_genre_df = actual_genres - predictions_df
error_genre_df$PredictionErrorOver = 0
error_genre_df$PredictionErrorUnder = 0
error_genre_df$PredictionTrueCorrect = 0
error_genre_df$PredictionNegativeCorrect = 0 
for(i in 1:nrow(error_genre_df)){
  r = as.numeric(error_genre_df[i,]); w = which(r == -1)
  error_genre_df$PredictionErrorOver[i] = length(w)
}
for(i in 1:nrow(error_genre_df)){
  r = as.numeric(error_genre_df[i,]); w = which(r == 10)
  error_genre_df$PredictionErrorUnder[i] = length(w)
}
for(i in 1:nrow(error_genre_df)){
  r = as.numeric(error_genre_df[i,]); w = which(r == 9)
  error_genre_df$PredictionTrueCorrect[i] = length(w)
}
for(i in 1:nrow(error_genre_df)){
  r = as.numeric(error_genre_df[i,]); w = which(r == 0)
  error_genre_df$PredictionNegativeCorrect[i] = length(w)
}
error_genre_df$Correct = error_genre_df$PredictionTrueCorrect / (error_genre_df$PredictionErrorUnder + error_genre_df$PredictionTrueCorrect)
over = mean(error_genre_df$PredictionErrorOver)
under = mean(error_genre_df$PredictionErrorUnder)
na_i = which(is.na(error_genre_df$Correct))
error_genre_df$Correct[na_i] = 0
genreall = length(which(error_genre_df$Correct == 1)) / nrow(error_genre_df)
genre1 = length(which(error_genre_df$Correct > 0)) / nrow(error_genre_df)
over_under = data.frame(genre1,genreall)
colnames(over_under) = c('At Least One Genre Predicted', 'All Genres Predicted')
over_under
```

##Conclusion

Recommendation systems have become the driving force behind many companies revenue streams. Collaborative filtering methods are common, but content based recommendations allow for diverse and transparent recommendations between business and user. More time spent tuning model parameters and investigating alternative methods of text clustering could lead to significant improvements in overall genre prediction accuracy. Aggregating synonyms and removing words of neutral sentiment have been shown to create more robust and accurate classification models. Future research using this data will be spent creating a content based model.


